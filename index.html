<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="description" content="Proprioceptive State Estimation for Amphibious Tactile Sensing">
  <meta name="keywords"
    content="Proprioception, shape reconstruction, soft robotics, state estimation, vision-based tactile sensing (VBTS)">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Proprioceptive State Estimation for Amphibious Tactile Sensing - SUSTech Design and Learning Research Group
  </title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="css/bulma.min.css">
  <link rel="stylesheet" href="css/bulma-carousel.min.css">
  <link rel="stylesheet" href="css/bulma-slider.min.css">
  <link rel="stylesheet" href="css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="css/index.css">
  <link rel="icon" href="images/icon.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="js/fontawesome.all.min.js"></script>
  <script src="js/bulma-carousel.min.js"></script>
  <script src="js/bulma-slider.min.js"></script>
  <script src="js/index.js"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Proprioceptive State Estimation for Amphibious Tactile Sensing</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://gabriel-ning.github.io">Ning Guo</a><sup>1,#</sup>,
              </span>
              <span class="author-block">
                <a href="https://hanxudong.cc">Xudong Han</a><sup>1,#</sup>,
              </span>
              <span class="author-block">
                <a href="https://orcid.org/0009-0006-0908-7992">Shuqiao Zhong</a><sup>2,#</sup>,
              </span>
              <span class="author-block">
                <a href="https://faculty.sustech.edu.cn/?tagid=zhouzy&lang=en">Zhiyuan Zhou</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://faculty.sustech.edu.cn/?tagid=linj&lang=en">Jian Lin</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://faculty.sustech.edu.cn/?tagid=daijs&lang=en">Jian S. Dai</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://maindl.ancorasir.com">Fang Wan</a><sup>3,</sup>*,
              </span>
              <span class="author-block">
                <a href="https://bionicdl.ancorasir.com">Chaoyang Song</a><sup>1,</sup>*
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Department of Mechanical and Energy Engineering,
              </span><br />
              <span class="author-block">
                <sup>2</sup>Department of Ocean Science and Engineering,
              </span><br />
              <span class="author-block">
                <sup>3</sup>School of Design,<br />Southern University of Science and Technology
              </span><br />
              <span class="author-block">
                <sup>#</sup>Equal Contribution,
              </span>
              <span class="author-block">
                *Corresponding authors
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://doi.org/10.1109/TRO.2024.3463509"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <div class="columns is-centered has-text-centered">
          <div class="column is-full-width">
            <video id="teaser" autoplay muted loop playsinline width="80%">
              <source src="videos/teaser.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              This paper presents a novel vision-based proprioception approach for a soft robotic finger that can
              estimate and reconstruct tactile interactions in terrestrial and aquatic environments. The key to this
              system lies in the finger's unique metamaterial structure, which facilitates omni-directional passive
              adaptation during grasping, protecting delicate objects across diverse scenarios. A compact in-finger
              camera captures high-framerate images of the finger's deformation during contact, extracting crucial
              tactile data in real time. We present a volumetric discretized model of the soft finger and use the
              geometry constraints captured by the camera to find the optimal estimation of the deformed shape. The
              approach is benchmarked using a motion capture system with sparse markers and a haptic device with dense
              measurements. Both results show state-of-the-art accuracies, with a median error of 1.96 mm for overall
              body deformation, corresponding to 2.1$\%$ of the finger's length. More importantly, the state estimation
              is robust in both on-land and underwater environments as we demonstrate its usage for underwater object
              shape sensing. This combination of passive adaptation and real-time tactile sensing paves the way for
              amphibious robotic grasping applications.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Soft Polyhedral Network with In-Finger Vision</h2>
          <div class="content has-text-justified">
            <p>
              In this study, we adopted our previous work in a class of <a
                href="https://doi.org/10.1177/02783649241238765">Soft Polyhedral Networks (SPNs)</a> with in-finger
              vision as the soft robotic finger. An ArUco tag is attached to the bottom side of a rigid plate
              mechanically fixed with the four lower crossbeams of the soft finger. A monocular RGB camera with a field
              of view (FOV) of 130Â° is fixed at the bottom inside a transparent support frame as in-finger vision,
              video-recording in a high frame rate of 120 FPS (frames per second) at 640x480 pixels resolution. When the
              soft robotic finger interacts with the external environment, live video streams captured by the in-finger
              vision camera provide real-time pose data of the ArUco tag as rigid-soft kinematics coupling constraints
              for the proprioceptive state estimation (PropSE) of the soft robotic finger. This marker-based in-finger
              vision design is equivalent to a miniature motion capture system, efficiently converting the soft robotic
              finger's spatial deformation into real-time 6D pose data.
            </p>
          </div>
          <div class="content">
            <img src="images/finger_design.png" width="500px" alt="finger design"></img>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Proprioceptive State Estimation of Soft Finger</h2>
          <h3 class="title is-4">Volumetric Modeling of Soft Deformation</h3>
          <div class="content has-text-justified">
            <p>
              Our proposed solution begins by formulating a volumetric model of the soft robotic finger in a 3D space
              filled with homogeneous elastic material. The visual observed marker area is approximated as aggregated
              multi-handles (AMH) on the mesh, and uniform rigid motion is applied to the AMH to drive the soft finger
              to a deformed configuration. The soft finger shape estimation can be translated into a constrained
              geometry optimization problem.
            </p>
          </div>
          <div class="content">
            <img src="images/deform_model.png" width="500px" alt="deform model"></img>
          </div>
          <br />
          <h3 class="title is-4">Comparison with Conventional Methods</h3>
          <div class="content has-text-justified">
            <p>
              We performed a comparative analysis with two widely adopted techniques, Abaqus and As-Rigid-As-Possible
              (ARAP), to showcase the efficacy of our shape estimation method. Results show that our method is 40 to 700
              times faster than Abaqus and 1 to 2 times faster than ARAP at different resolutions, and our method's mean
              error decreases significantly, from 0.346 mm to 0.086 mm, as the number of elements increases, showing
              significant advantages over Abaqus and ARAP.
            </p>
          </div>
          <div class="content">
            <img src="images/comparison_abq_arap.png" width="500px" alt="comparison with abaqus and arap"></img>
          </div>
          <br />
          <h3 class="title is-4">Evaluation with Motion Capture System</h3>
          <div class="content has-text-justified">
            <p>
              To evaluate the performance of the deformation estimation, the soft robotic finger was mounted on a
              three-axis motion platform for interactive deformation estimation. A motion capture system (Mars2H, Nokov)
              was used to track finger deformations. There were nine markers attached to the soft finger with rigid
              links. Among them, six markers were divided into three pairs, which were rigidly attached to the
              fingertip, the first layer, and the second layer of the soft finger, respectively. The other three markers
              were attached to the platform and used as the reference reading to align the motion capture system's
              reference frame with the platform's coordinate frame.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline width="100%">
              <source src="videos/evaluation_mocap.mp4" type="video/mp4">
            </video>
          </div>
          <div class="content has-text-justified">
            <p>
              We visualize the error distribution with 3k pairs of the estimated and ground truth positions of the
              markers. The norm of the total error is within 3 mm, while error distribution along each axis is centered
              around the (-2, 2) mm range. As the marker prediction model comprises calibration and geometric
              optimization, the error distribution of six sparse markers may only partially validate the proposed
              method, leading to the next experiment.
            </p>
          </div>
          <div class="content">
            <img src="images/evaluation_mocap.png" width="900px" alt="evaluation with mocap"></img>
          </div>
          <br />
          <h3 class="title is-4">Evaluation with Touch Haptic Device</h3>
          <div class="content has-text-justified">
            <p>
              We designed another validation experiment using the pen-nib's position of a haptic device (Touch, 3D
              Systems) as ground truth measurement. an operator holding the pen-nib initiated contact at a random point
              on the soft robotic finger by pushing it five times. Fifty points were sampled, spreading over half of the
              soft robotic finger with recorded pen-nib position and the corresponding point of contact on the estimated
              deformation in the mesh model.
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline width="100%">
              <source src="videos/evaluation_touch.mp4" type="video/mp4">
            </video>
          </div>
          <div class="content has-text-justified">
            <p>
              Similar to the calibration process when using the motion capture system, we solve the barycentric
              coordinates using the initial contact position of pen-nib and the undeformed vertex position of the
              tetrahedron nearest to the contact point. Due to the variations among the pushing trajectories among the
              three locations, the errors are slightly different, but all lie within a 2.5 mm range. The haptic device
              measurements cover an extensive portion of the soft robotic finger, revealing further details regarding
              the spatial distribution of the estimation errors. We visualize the mean errors of deformation estimation
              evaluated at the fifty randomly selected contact locations. Contact locations near the observed AMH
              constraint are expected to exhibit fewer errors due to penalized computation near this region during
              deformation optimization. The median of estimated error for the whole-body deformation is 1.96 mm,
              corresponding to 2.1% of the finger's length.
            </p>
          </div>
          <div class="content">
            <img src="images/evaluation_touch.png" width="100%" alt="evaluation with touch"></img>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Amphibious Tactile Sensing</h2>
          <div class="content has-text-justified">
            <p>
              <b>Multi-modal Variational Autoencoder (MVAE)</b>
              is developed to learn the morphing primitives of SPMs, which include
              the multi-modal information representing SPMs' behaviours.
              The architecture of MVAE is shown below:
            </p>
          </div>
          <div class="content">
            <img src="images/mvae.jpg" width="90%" alt="mvae"></img>
          </div>
          <div class="content has-text-justified">
            <p>
              To train the generative network, we collected a large dataset (100,000
              frames for each type of SPM and a total of 600,000 frames) through FEA
              in Abaqus.
              The forces <b>X</b><sub>force</sub> on the bottom surface and the
              displacements <b>X</b><sub>node</sub> of nodes on the whole surface are
              collected with a series of random poses <b>X</b><sub>pose</sub> on the
              top surface, corresponding to the marker poses.
            </p>
            <p>
              Through MVAE training, all kinds of modal information of SPM are
              aligned into a series of latent distributions, which are subsequently
              utilized to generate various modal information under different states.
              The correlation map and the chord diagram of the latent vectors show
              that there are 6 latent distributions performing more important roles,
              which we called as <b>morphing primitives</b>.
            </p>
          </div>
          <div class="content">
            <img src="images/mp.jpg" width="90%" alt="morphing primitives"></img>
          </div>
          <div class="content has-text-justified">
            <p>
              By changing the values of morphing primitives, the generated shapes are
              deformed in specific manifolds. The deformation cannot be characterized
              as a typically single motion within the Euclidean space, but it
              presents a novel perspective to comprehend the intrinsic dimensionality
              of the SPMs' multi-modal information. The generated shapes with
              different values of morphing primitives are shown below:
            </p>
          </div>
          <video id="teaser" autoplay muted loop playsinline width="80%">
            <source src="videos/mp_var.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Sim-to-Real Transferability</h2>
          <div class="content has-text-justified">
            <p>
              To evaluate the performance of the MVAE on estimating proprioceptive
              information of the SPM in the real world, we developed an evaluation
              environment, including a robotic arm, a 3-dimensional (3D) scanner, a
              camera, a pose tag, a force/torque sensor, four LED panels, and the SPM.
            </p>
          </div>
          <div class="content">
            <img src="images/sim2real_setup.jpg" width="70%" alt="sim2real evalution setup"></img>
          </div>
          <div class="content has-text-justified">
            <p>
              We compared the shapes estimated by MVAE with the ground truth
              measured by the 3D scanner under 50 different poses for each type of
              SPM.
              We also compared the forces with those measured by the force/torque
              sensor under manual movements during a period of 30 seconds.
              The results show that the estimated shapes and forces are highly
              consistent with the ground truth.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column">
              <img src="images/sim2real_shape_force.png" width="100%" alt=""></img>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Twin motion with tendon-driven platform</h2>
          <div class="content has-text-justified">
            <p>
              To enhance SPM with controllable motion capability, we develop a
              tendon-driven platform, which is composed of a mounting base,
              actuators, tendons, driving drums, and pulleys.
              The SPM is capable of 6-DOF motion by mounted and driven by this
              platform.
              We tested the driving accuracy of the platform on each dimension by
              comparing the input reference and the actual motion measured by motion
              capture system (MoCap).
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline controls width="100%">
                <source src="videos/driving_trans.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline controls width="100%">
                <source src="videos/driving_rot.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="content has-text-justified">
            <p>
              We realized the twin motion among the manual SPM, the active SPM, and
              the digital SPM.
              The manual SPM is deformed by human hands, the active SPM is controlled
              by the tendon-driven platform, and the digital SPM is to visualize the
              proprioceptive information of the SPM.
            </p>
          </div>
          <div class="content">
            <img src="images/twin_motion.png" width="90%" alt="twin motion"></img>
          </div>
          <div class="content has-text-justified">
            <p>
              We moved the manual SPM by hand along four trajectories, including
              circle, square, four-leaved rose, and spiral, and the active SPM is
              moved synchronously by the tendon-driven platform.
              The SPM's trajectories are plotted in x-y plane based on a reference
              point, which is the geometric center of markers.
            </p>
          </div>
          <div class="columns is-centered">
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline controls width="100%">
                <source src="videos/tm_circle.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline controls width="100%">
                <source src="videos/tm_square.mp4" type="video/mp4">
              </video>
            </div>
          </div>
          <div class="columns is-centered">
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline controls width="100%">
                <source src="videos/tm_rose.mp4" type="video/mp4">
              </video>
            </div>
            <div class="column">
              <video id="teaser" autoplay muted loop playsinline controls width="100%">
                <source src="videos/tm_spiral.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Soft-rigid Hybrid Modular Arm</h2>
          <div class="content has-text-justified">
            <p>
              XXX
            </p>
          </div>
          <div class="content">
            <img src="images/arm_pl.jpg" width="90%" alt="arm pl"></img>
          </div>
          <div class="content has-text-justified">
            <p>
              XXX
            </p>
          </div>
          <div class="content">
            <video id="teaser" autoplay muted loop playsinline controls width="90%">
              <source src="videos/arm.mp4" type="video/mp4">
            </video>
          </div>
          <div class="content has-text-justified">
            <p>
              XXX
            </p>
          </div>
          <div class="content">
            <img src="images/arm_res.jpg" width="90%" alt="arm result"></img>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
        <code>
@article{han2024spm,
    author  = {Han, Xudong and Guo, Ning and Xu, Ronghan and Liu, Xiaobo and Dai, Jian S. and Hong, Wei and Wan, Fang and Song, Chaoyang},
    title   = {Soft Proprioceptive Module},
    journal = {arXi preprint arXiv:2409},
    year    = {2024}
}
        </code>
      </pre>
    </div>
  </section>

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website template credit to
              <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>,
              and is licensed under a
              <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>
</body>

</html>